import streamlit as st
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch

@st.cache_resource
def load_model():
    tokenizer = GPT2Tokenizer.from_pretrained("robiulhasanjisan88/Bangla-QA-BERT")
    model = GPT2LMHeadModel.from_pretrained("robiulhasanjisan88/Bangla-QA-BERT")
    model.eval()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    return tokenizer, model, device

tokenizer, model, device = load_model()

st.set_page_config(page_title="Chemistry AI Assistant", layout="centered")

# Dark theme CSS
st.markdown("""
<style>
    .stApp {
        background-color: #0d1117;
        color: #e6edf3;
        font-family: 'Segoe UI', sans-serif;
    }
    .chat-bubble-user {
        background-color: #1f6feb;
        color: white;
        padding: 12px 18px;
        border-radius: 12px;
        margin: 10px 0;
        text-align: right;
        max-width: 80%;
        word-wrap: break-word;
    }
    .chat-bubble-ai {
        background-color: #161b22;
        color: #e6edf3;
        border: 1px solid #30363d;
        padding: 12px 18px;
        border-radius: 12px;
        margin: 10px 0;
        text-align: left;
        max-width: 80%;
        word-wrap: break-word;
    }
    .footer {
        margin-top: 30px;
        font-size: 13px;
        text-align: center;
        color: #8b949e;
    }
    .stTextInput>div>div>input {
        background-color: #21262d;
        color: #f0f6fc;
        border: 1px solid #30363d;
        border-radius: 8px;
    }
    .stButton>button {
        background-color: #238636;
        color: white;
        border: none;
        border-radius: 8px;
        padding: 0.5rem 1rem;
    }
    .stButton>button:hover {
        background-color: #2ea043;
    }
</style>
""", unsafe_allow_html=True)

st.title("Chemistry AI Assistant")
st.write("Ask a chemistry question and get answers generated by the AI model.")

# Initialize chat history
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Clear chat button
if st.button("Clear Chat History"):
    st.session_state.chat_history = []

# Input box
user_input = st.text_input("Your Question:", placeholder="e.g. What is an atom?")

if user_input:
    st.session_state.chat_history.append(("user", user_input))

    # Encode and generate
    input_ids = tokenizer.encode(user_input, return_tensors="pt").to(device)

    with torch.no_grad():
        output_ids = model.generate(
            input_ids,
            max_length=200,
            num_return_sequences=1,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            temperature=0.7,
            top_p=0.95,
            repetition_penalty=1.2,
            do_sample=True
        )
    answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)

    # Remove user input from answer (to only keep AI response)
    answer = answer[len(user_input):].strip()
    if not answer:
        answer = "Sorry, I couldn't generate an answer."

    st.session_state.chat_history.append(("ai", answer))

# Show chat history (latest at bottom)
for sender, msg in st.session_state.chat_history:
    bubble_class = "chat-bubble-user" if sender == "user" else "chat-bubble-ai"
    st.markdown(f"<div class='{bubble_class}'>{msg}</div>", unsafe_allow_html=True)

# Footer
st.markdown("<div class='footer'>Powered by Hugging Face & GPT-2</div>", unsafe_allow_html=True)
